{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e6c15b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4832630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad2fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"A2Q1.csv\")\n",
    "df2 = pd.read_csv(\"A2Q2test.csv\", header=None)\n",
    "df2_test = pd.read_csv(\"A2Q2train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47efa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21302</td>\n",
       "      <td>0.446210</td>\n",
       "      <td>0.51432</td>\n",
       "      <td>0.219750</td>\n",
       "      <td>0.49544</td>\n",
       "      <td>0.46367</td>\n",
       "      <td>0.26007</td>\n",
       "      <td>0.28137</td>\n",
       "      <td>0.983820</td>\n",
       "      <td>0.00777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92910</td>\n",
       "      <td>0.28669</td>\n",
       "      <td>0.45841</td>\n",
       "      <td>0.788740</td>\n",
       "      <td>0.041794</td>\n",
       "      <td>0.58736</td>\n",
       "      <td>0.90491</td>\n",
       "      <td>0.91718</td>\n",
       "      <td>0.68321</td>\n",
       "      <td>2.0121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.32563</td>\n",
       "      <td>0.703990</td>\n",
       "      <td>0.31970</td>\n",
       "      <td>0.045240</td>\n",
       "      <td>0.86968</td>\n",
       "      <td>0.13136</td>\n",
       "      <td>0.47664</td>\n",
       "      <td>0.26632</td>\n",
       "      <td>0.334520</td>\n",
       "      <td>0.81200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51868</td>\n",
       "      <td>0.25546</td>\n",
       "      <td>0.29498</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.169970</td>\n",
       "      <td>0.92325</td>\n",
       "      <td>0.42134</td>\n",
       "      <td>0.90906</td>\n",
       "      <td>0.61539</td>\n",
       "      <td>1.8164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.49539</td>\n",
       "      <td>0.623570</td>\n",
       "      <td>0.58121</td>\n",
       "      <td>0.879160</td>\n",
       "      <td>0.93096</td>\n",
       "      <td>0.20304</td>\n",
       "      <td>0.35752</td>\n",
       "      <td>0.31232</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.46389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85782</td>\n",
       "      <td>0.50530</td>\n",
       "      <td>0.32361</td>\n",
       "      <td>0.693090</td>\n",
       "      <td>0.690430</td>\n",
       "      <td>0.01604</td>\n",
       "      <td>0.46741</td>\n",
       "      <td>0.67303</td>\n",
       "      <td>0.60459</td>\n",
       "      <td>1.6435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.88875</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.87982</td>\n",
       "      <td>0.858160</td>\n",
       "      <td>0.79392</td>\n",
       "      <td>0.79239</td>\n",
       "      <td>0.76522</td>\n",
       "      <td>0.85145</td>\n",
       "      <td>0.233610</td>\n",
       "      <td>0.57924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25535</td>\n",
       "      <td>0.38341</td>\n",
       "      <td>0.55130</td>\n",
       "      <td>0.727730</td>\n",
       "      <td>0.676890</td>\n",
       "      <td>0.84605</td>\n",
       "      <td>0.61116</td>\n",
       "      <td>0.58514</td>\n",
       "      <td>0.41724</td>\n",
       "      <td>2.7451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15178</td>\n",
       "      <td>0.540760</td>\n",
       "      <td>0.26896</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.10102</td>\n",
       "      <td>0.34208</td>\n",
       "      <td>0.86855</td>\n",
       "      <td>0.32698</td>\n",
       "      <td>0.285460</td>\n",
       "      <td>0.80503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54955</td>\n",
       "      <td>0.12932</td>\n",
       "      <td>0.94848</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.268010</td>\n",
       "      <td>0.27125</td>\n",
       "      <td>0.53535</td>\n",
       "      <td>0.91872</td>\n",
       "      <td>0.16801</td>\n",
       "      <td>1.8932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1        2         3        4        5        6        7    \\\n",
       "0  0.21302  0.446210  0.51432  0.219750  0.49544  0.46367  0.26007  0.28137   \n",
       "1  0.32563  0.703990  0.31970  0.045240  0.86968  0.13136  0.47664  0.26632   \n",
       "2  0.49539  0.623570  0.58121  0.879160  0.93096  0.20304  0.35752  0.31232   \n",
       "3  0.88875  0.013187  0.87982  0.858160  0.79392  0.79239  0.76522  0.85145   \n",
       "4  0.15178  0.540760  0.26896  0.006752  0.10102  0.34208  0.86855  0.32698   \n",
       "\n",
       "        8        9    ...      91       92       93        94        95   \\\n",
       "0  0.983820  0.00777  ...  0.92910  0.28669  0.45841  0.788740  0.041794   \n",
       "1  0.334520  0.81200  ...  0.51868  0.25546  0.29498  0.008869  0.169970   \n",
       "2  0.066225  0.46389  ...  0.85782  0.50530  0.32361  0.693090  0.690430   \n",
       "3  0.233610  0.57924  ...  0.25535  0.38341  0.55130  0.727730  0.676890   \n",
       "4  0.285460  0.80503  ...  0.54955  0.12932  0.94848  0.326100  0.268010   \n",
       "\n",
       "       96       97       98       99      100  \n",
       "0  0.58736  0.90491  0.91718  0.68321  2.0121  \n",
       "1  0.92325  0.42134  0.90906  0.61539  1.8164  \n",
       "2  0.01604  0.46741  0.67303  0.60459  1.6435  \n",
       "3  0.84605  0.61116  0.58514  0.41724  2.7451  \n",
       "4  0.27125  0.53535  0.91872  0.16801  1.8932  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd541ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.58351</td>\n",
       "      <td>0.37939</td>\n",
       "      <td>0.83975</td>\n",
       "      <td>0.086861</td>\n",
       "      <td>0.61192</td>\n",
       "      <td>0.39203</td>\n",
       "      <td>0.189640</td>\n",
       "      <td>0.585090</td>\n",
       "      <td>0.89494</td>\n",
       "      <td>0.393850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51706</td>\n",
       "      <td>0.20535</td>\n",
       "      <td>0.829990</td>\n",
       "      <td>0.24995</td>\n",
       "      <td>0.31980</td>\n",
       "      <td>0.19674</td>\n",
       "      <td>0.958550</td>\n",
       "      <td>0.84948</td>\n",
       "      <td>0.92369</td>\n",
       "      <td>1.9741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86622</td>\n",
       "      <td>0.45135</td>\n",
       "      <td>0.93479</td>\n",
       "      <td>0.235810</td>\n",
       "      <td>0.50488</td>\n",
       "      <td>0.49742</td>\n",
       "      <td>0.117220</td>\n",
       "      <td>0.574880</td>\n",
       "      <td>0.71051</td>\n",
       "      <td>0.336890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27412</td>\n",
       "      <td>0.46065</td>\n",
       "      <td>0.655440</td>\n",
       "      <td>0.56656</td>\n",
       "      <td>0.49062</td>\n",
       "      <td>0.19920</td>\n",
       "      <td>0.320680</td>\n",
       "      <td>0.41419</td>\n",
       "      <td>0.83023</td>\n",
       "      <td>2.3354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.39142</td>\n",
       "      <td>0.91995</td>\n",
       "      <td>0.13873</td>\n",
       "      <td>0.991810</td>\n",
       "      <td>0.54367</td>\n",
       "      <td>0.98449</td>\n",
       "      <td>0.340260</td>\n",
       "      <td>0.206470</td>\n",
       "      <td>0.40131</td>\n",
       "      <td>0.128990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27247</td>\n",
       "      <td>0.38911</td>\n",
       "      <td>0.799490</td>\n",
       "      <td>0.57753</td>\n",
       "      <td>0.86682</td>\n",
       "      <td>0.69741</td>\n",
       "      <td>0.628630</td>\n",
       "      <td>0.29479</td>\n",
       "      <td>0.54280</td>\n",
       "      <td>2.2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.79553</td>\n",
       "      <td>0.27854</td>\n",
       "      <td>0.22890</td>\n",
       "      <td>0.789290</td>\n",
       "      <td>0.49222</td>\n",
       "      <td>0.67084</td>\n",
       "      <td>0.769410</td>\n",
       "      <td>0.057104</td>\n",
       "      <td>0.20393</td>\n",
       "      <td>0.086936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24901</td>\n",
       "      <td>0.65976</td>\n",
       "      <td>0.943350</td>\n",
       "      <td>0.41246</td>\n",
       "      <td>0.92715</td>\n",
       "      <td>0.11015</td>\n",
       "      <td>0.101310</td>\n",
       "      <td>0.66801</td>\n",
       "      <td>0.34573</td>\n",
       "      <td>2.0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.65223</td>\n",
       "      <td>0.96665</td>\n",
       "      <td>0.17985</td>\n",
       "      <td>0.354060</td>\n",
       "      <td>0.84042</td>\n",
       "      <td>0.92891</td>\n",
       "      <td>0.014597</td>\n",
       "      <td>0.229530</td>\n",
       "      <td>0.43920</td>\n",
       "      <td>0.548940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.93277</td>\n",
       "      <td>0.23674</td>\n",
       "      <td>0.027077</td>\n",
       "      <td>0.27111</td>\n",
       "      <td>0.23366</td>\n",
       "      <td>0.56356</td>\n",
       "      <td>0.047289</td>\n",
       "      <td>0.62792</td>\n",
       "      <td>0.97930</td>\n",
       "      <td>2.2053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2         3        4        5         6         7    \\\n",
       "0  0.58351  0.37939  0.83975  0.086861  0.61192  0.39203  0.189640  0.585090   \n",
       "1  0.86622  0.45135  0.93479  0.235810  0.50488  0.49742  0.117220  0.574880   \n",
       "2  0.39142  0.91995  0.13873  0.991810  0.54367  0.98449  0.340260  0.206470   \n",
       "3  0.79553  0.27854  0.22890  0.789290  0.49222  0.67084  0.769410  0.057104   \n",
       "4  0.65223  0.96665  0.17985  0.354060  0.84042  0.92891  0.014597  0.229530   \n",
       "\n",
       "       8         9    ...      91       92        93       94       95   \\\n",
       "0  0.89494  0.393850  ...  0.51706  0.20535  0.829990  0.24995  0.31980   \n",
       "1  0.71051  0.336890  ...  0.27412  0.46065  0.655440  0.56656  0.49062   \n",
       "2  0.40131  0.128990  ...  0.27247  0.38911  0.799490  0.57753  0.86682   \n",
       "3  0.20393  0.086936  ...  0.24901  0.65976  0.943350  0.41246  0.92715   \n",
       "4  0.43920  0.548940  ...  0.93277  0.23674  0.027077  0.27111  0.23366   \n",
       "\n",
       "       96        97       98       99      100  \n",
       "0  0.19674  0.958550  0.84948  0.92369  1.9741  \n",
       "1  0.19920  0.320680  0.41419  0.83023  2.3354  \n",
       "2  0.69741  0.628630  0.29479  0.54280  2.2053  \n",
       "3  0.11015  0.101310  0.66801  0.34573  2.0400  \n",
       "4  0.56356  0.047289  0.62792  0.97930  2.2053  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32622436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c79a1",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdacf796",
   "metadata": {},
   "source": [
    "The analytical or closed-form solution for the least squares weight vector, $w_{ML}$, is found by solving the normal equation for $w$:\n",
    "\n",
    "$$\n",
    "w_{ML} = (X^{T} X)^{-1} X^{T} y\n",
    "$$\n",
    "\n",
    "This solution requires the matrix $(X^{T} X)$ to be invertible. If it's not invertible (i.e., it's a singular matrix), a unique analytical solution does not exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e21bd",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f6fe9",
   "metadata": {},
   "source": [
    "## My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb20292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def Fit(self,Xo,Y, method = None ,regularize = None, alpha = 0.01, max_iter = 10000, lambda_ = 1, capture_cost = False, tol = 1e-6,  batch_size = 5, epoch = 20):\n",
    "        X = Xo.copy()\n",
    "        X.insert(0, \"bias\", 1)\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        Y = Y.to_numpy().reshape(-1, 1)\n",
    "        m = len(Y)\n",
    "        \n",
    "        if method == 'Stoc_GD':\n",
    "            n = X.shape[1]\n",
    "            rng = np.random.default_rng()\n",
    "            self.w = rng.standard_normal((n,1))\n",
    "            self.w_history = []\n",
    "            for _ in range(max_iter+1):\n",
    "                y_pred = X @ self.w\n",
    "                if capture_cost == True:\n",
    "                    self.cost = np.sum((Y - y_pred)**2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "                self.diff = (1/m) * ((X.T @ (X @ self.w - Y)))\n",
    "                self.reg = (lambda_/m) * self.w\n",
    "                \n",
    "                if regularize == \"Ridge\":\n",
    "                    grad = self.diff + self.reg\n",
    "                else:\n",
    "                    grad = self.diff\n",
    "                self.w = self.w - alpha * grad\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            if capture_cost == True:\n",
    "                return self.w, self.w_history\n",
    "            else:\n",
    "                return self.w\n",
    "            \n",
    "        elif method == 'MiniBatch_GD':\n",
    "            def List_mean(a):\n",
    "                avg = sum(a) / len(a)\n",
    "                return avg\n",
    "\n",
    "            n = X.shape[1]\n",
    "            rng = np.random.default_rng()\n",
    "            self.w = rng.standard_normal((n,1))\n",
    "            self.w_history = []\n",
    "            list_ = []\n",
    "\n",
    "            \n",
    "            for i in range(0,max_iter+1):\n",
    "                y_pred = X @ self.w\n",
    "                if capture_cost == True:\n",
    "                    self.cost = np.sum((Y - y_pred)**2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "                self.diff = (1/m) * ((X.T @ (X @ self.w - Y)))\n",
    "                self.reg = (lambda_/m) * self.w\n",
    "                \n",
    "                if regularize == \"Ridge\":\n",
    "                    grad = self.diff + self.reg\n",
    "                else:\n",
    "                    grad = self.diff\n",
    "\n",
    "                if (i+1)%batch_size != 0:\n",
    "                    list_.append(grad)\n",
    "                else:\n",
    "                    self.w = self.w - alpha * List_mean(list_)\n",
    "                    list_.clear()\n",
    "\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            if capture_cost == True:\n",
    "                return self.w, self.w_history\n",
    "            else:\n",
    "                return self.w\n",
    "\n",
    "        elif method == 'Batch_GD':\n",
    "\n",
    "            n = X.shape[1]\n",
    "            rng = np.random.default_rng()\n",
    "            self.w = rng.standard_normal((n,1))\n",
    "            self.w_history = []\n",
    "            list_ = []\n",
    "\n",
    "            for _ in range(epoch):\n",
    "                for i in range(0,max_iter+1):\n",
    "                    y_pred = X @ self.w\n",
    "                    if capture_cost == True:\n",
    "                        self.cost = np.sum((Y - y_pred)**2)\n",
    "                        self.w_history.append(self.w.copy())\n",
    "                    self.diff = (1/m) * ((X.T @ (X @ self.w - Y)))\n",
    "                    self.reg = (lambda_/m) * self.w\n",
    "                    \n",
    "                    if regularize == \"Ridge\":\n",
    "                        grad = self.diff + self.reg\n",
    "                    else:\n",
    "                        grad = self.diff\n",
    "\n",
    "                    list_.append(grad)\n",
    "                    \n",
    "                    if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                        break\n",
    "                    \n",
    "                self.w = self.w - alpha * List_mean(list_)\n",
    "\n",
    "            if capture_cost == True:\n",
    "                return self.w, self.w_history\n",
    "            else:\n",
    "                return self.w\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            tra = X.T\n",
    "            if np.linalg.det(tra @ X) == 0:\n",
    "                raise ValueError(\"Singular Matrix\")\n",
    "            else:\n",
    "                self.w = (np.linalg.inv(tra @ X)) @ tra @ Y\n",
    "                return self.w\n",
    "            \n",
    "    def Test(self, X_test):\n",
    "        if self.w is None:\n",
    "            raise Exception(\"Training Not yet Done\")\n",
    "        else:\n",
    "            X_test = X_test.copy()\n",
    "            X_test.insert(0, \"bias\", 1)\n",
    "            X_test = X_test.to_numpy()\n",
    "            y_pred = X_test @ self.w\n",
    "            return y_pred\n",
    "    \n",
    "    def Eval(self, Y_Pred, Y_test):\n",
    "        \n",
    "        Y_Pred = np.array(Y_Pred).reshape(-1)\n",
    "        Y_test = np.array(Y_test).reshape(-1)\n",
    "\n",
    "        meann = np.mean(Y_test)\n",
    "        ss_res = np.sum((Y_test - Y_Pred)**2)\n",
    "        ss_tot = np.sum((Y_test - meann)**2)\n",
    "\n",
    "        R = 1 - (ss_res/ss_tot)\n",
    "        return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb13d3",
   "metadata": {},
   "source": [
    "## My Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "376f06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def Fit(self, Xo, Y, method=None, regularize=None, alpha=0.001, max_iter=10000, lambda_=1, capture_cost=False, tol=1e-6, batch_size=100, epoch=20):\n",
    "        X = Xo.copy()\n",
    "        X.insert(0, \"bias\", 1)\n",
    "        X = X.to_numpy()\n",
    "        Y = Y.to_numpy().reshape(-1, 1)\n",
    "        m = len(Y)\n",
    "        self.w = np.zeros((n, 1))\n",
    "        self.w_history = []\n",
    "        n = X.shape[1]\n",
    "\n",
    "        def List_mean(a):\n",
    "            return sum(a) / len(a)\n",
    "\n",
    "        if method == 'Stoc_GD':\n",
    "            for _ in range(max_iter):\n",
    "                i = rng.integers(0, m)\n",
    "                Xi = X[i:i+1]\n",
    "                Yi = Y[i:i+1]\n",
    "\n",
    "                y_pred = Xi @ self.w\n",
    "                if capture_cost:\n",
    "                    full_pred = X @ self.w\n",
    "                    self.cost = np.sum((Y - full_pred) ** 2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "\n",
    "                grad = Xi.T @ (Xi @ self.w - Yi)\n",
    "\n",
    "                if regularize == \"Ridge\":\n",
    "                    reg_term = np.zeros_like(self.w)\n",
    "                    reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                    grad += reg_term\n",
    "\n",
    "                self.w -= alpha * grad\n",
    "\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        elif method == 'MiniBatch_GD':\n",
    "            for epoch_idx in range(epoch):\n",
    "                rng = np.random.default_rng()\n",
    "                indices = rng.permutation(m)\n",
    "                X_shuffled = X[indices]\n",
    "                Y_shuffled = Y[indices]\n",
    "\n",
    "                for start in range(0, m, batch_size):\n",
    "                    end = start + batch_size\n",
    "                    Xi = X_shuffled[start:end]\n",
    "                    Yi = Y_shuffled[start:end]\n",
    "\n",
    "                    y_pred = Xi @ self.w\n",
    "                    if capture_cost:\n",
    "                        full_pred = X @ self.w\n",
    "                        self.cost = np.sum((Y - full_pred) ** 2)\n",
    "                        self.w_history.append(self.w.copy())\n",
    "\n",
    "                    grad = Xi.T @ (Xi @ self.w - Yi) / Xi.shape[0]\n",
    "\n",
    "                    if regularize == \"Ridge\":\n",
    "                        reg_term = np.zeros_like(self.w)\n",
    "                        reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                        grad += reg_term\n",
    "\n",
    "                    self.w -= alpha * grad\n",
    "\n",
    "                    if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                        break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        elif method == 'Batch_GD':\n",
    "            for epoch_idx in range(epoch):\n",
    "                y_pred = X @ self.w\n",
    "                if capture_cost:\n",
    "                    self.cost = np.sum((Y - y_pred) ** 2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "\n",
    "                grad = (X.T @ (X @ self.w - Y)) / m\n",
    "\n",
    "                if regularize == \"Ridge\":\n",
    "                    reg_term = np.zeros_like(self.w)\n",
    "                    reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                    grad += reg_term\n",
    "\n",
    "                self.w -= alpha * grad\n",
    "\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        else:\n",
    "            tra = X.T\n",
    "            try:\n",
    "                self.w = np.linalg.inv(tra @ X) @ tra @ Y\n",
    "            except np.linalg.LinAlgError:\n",
    "                self.w = np.linalg.pinv(tra @ X) @ tra @ Y\n",
    "            return self.w\n",
    "\n",
    "    def Test(self, X_test):\n",
    "        if self.w is None:\n",
    "            raise Exception(\"Training Not yet Done\")\n",
    "        X_test = X_test.copy()\n",
    "        X_test.insert(0, \"bias\", 1)\n",
    "        X_test = X_test.to_numpy()\n",
    "        y_pred = X_test @ self.w\n",
    "        return y_pred\n",
    "\n",
    "    def Eval(self, Y_Pred, Y_test):\n",
    "        R = 0\n",
    "        Y_Pred = np.array(Y_Pred).reshape(-1)\n",
    "        Y_test = np.array(Y_test).reshape(-1)\n",
    "        y_mean = np.mean(Y_test)\n",
    "        ss_res = np.sum((Y_test - Y_Pred) ** 2)\n",
    "        ss_tot = np.sum((Y_test - y_mean) ** 2)\n",
    "        R = 1 - (ss_res / ss_tot)\n",
    "        return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "880d047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LinReg():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.w_history = []\n",
    "        self.cost = None\n",
    "\n",
    "    def Fit(self, Xo, Y, method=None, regularize=None, alpha=0.001, max_iter=10000, lambda_=1, capture_cost=False, tol=1e-6, batch_size=100, epoch=20):\n",
    "        X = Xo.copy()\n",
    "        X.insert(0, \"bias\", 1)\n",
    "        X = X.to_numpy()\n",
    "        Y = Y.to_numpy().reshape(-1, 1)\n",
    "        m, n = X.shape\n",
    "\n",
    "        self.w = np.zeros((n, 1))\n",
    "        self.w_history = []\n",
    "\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "        def List_mean(a):\n",
    "            return sum(a) / len(a)\n",
    "\n",
    "        # ---------------- Stochastic Gradient Descent ----------------\n",
    "        if method == 'Stoc_GD':\n",
    "            for _ in range(max_iter):\n",
    "                i = rng.integers(0, m)\n",
    "                Xi = X[i:i+1]\n",
    "                Yi = Y[i:i+1]\n",
    "\n",
    "                grad = Xi.T @ (Xi @ self.w - Yi)\n",
    "\n",
    "                if regularize == \"Ridge\":\n",
    "                    reg_term = np.zeros_like(self.w)\n",
    "                    reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                    grad += reg_term\n",
    "\n",
    "                self.w -= alpha * grad\n",
    "\n",
    "                if capture_cost:\n",
    "                    full_pred = X @ self.w\n",
    "                    self.cost = np.sum((Y - full_pred) ** 2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        # ---------------- Mini-Batch Gradient Descent ----------------\n",
    "        elif method == 'MiniBatch_GD':\n",
    "            for epoch_idx in range(epoch):\n",
    "                indices = rng.permutation(m)\n",
    "                X_shuffled = X[indices]\n",
    "                Y_shuffled = Y[indices]\n",
    "\n",
    "                for start in range(0, m, batch_size):\n",
    "                    end = start + batch_size\n",
    "                    Xi = X_shuffled[start:end]\n",
    "                    Yi = Y_shuffled[start:end]\n",
    "\n",
    "                    grad = Xi.T @ (Xi @ self.w - Yi) / Xi.shape[0]\n",
    "\n",
    "                    if regularize == \"Ridge\":\n",
    "                        reg_term = np.zeros_like(self.w)\n",
    "                        reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                        grad += reg_term\n",
    "\n",
    "                    self.w -= alpha * grad\n",
    "\n",
    "                    if capture_cost:\n",
    "                        full_pred = X @ self.w\n",
    "                        self.cost = np.sum((Y - full_pred) ** 2)\n",
    "                        self.w_history.append(self.w.copy())\n",
    "\n",
    "                    if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                        break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        # ---------------- Batch Gradient Descent ----------------\n",
    "        elif method == 'Batch_GD':\n",
    "            for epoch_idx in range(epoch):\n",
    "                grad = (X.T @ (X @ self.w - Y)) / m\n",
    "\n",
    "                if regularize == \"Ridge\":\n",
    "                    reg_term = np.zeros_like(self.w)\n",
    "                    reg_term[1:] = (lambda_ / m) * self.w[1:]\n",
    "                    grad += reg_term\n",
    "\n",
    "                self.w -= alpha * grad\n",
    "\n",
    "                if capture_cost:\n",
    "                    full_pred = X @ self.w\n",
    "                    self.cost = np.sum((Y - full_pred) ** 2)\n",
    "                    self.w_history.append(self.w.copy())\n",
    "\n",
    "                if len(self.w_history) > 1 and np.linalg.norm(self.w_history[-1] - self.w_history[-2]) < tol:\n",
    "                    break\n",
    "\n",
    "            return (self.w, self.w_history) if capture_cost else self.w\n",
    "\n",
    "        # ---------------- Closed-form Solution ----------------\n",
    "        else:\n",
    "            tra = X.T\n",
    "            try:\n",
    "                self.w = np.linalg.inv(tra @ X) @ tra @ Y\n",
    "            except np.linalg.LinAlgError:\n",
    "                self.w = np.linalg.pinv(tra @ X) @ tra @ Y\n",
    "            return self.w\n",
    "\n",
    "    # ---------------- Prediction ----------------\n",
    "    def Test(self, X_test):\n",
    "        if self.w is None:\n",
    "            raise Exception(\"Training Not yet Done\")\n",
    "        X_test = X_test.copy()\n",
    "        X_test.insert(0, \"bias\", 1)\n",
    "        X_test = X_test.to_numpy()\n",
    "        y_pred = X_test @ self.w\n",
    "        return y_pred\n",
    "\n",
    "    # ---------------- R^2 Evaluation ----------------\n",
    "    def Eval(self, Y_Pred, Y_test):\n",
    "        Y_Pred = np.array(Y_Pred).reshape(-1)\n",
    "        Y_test = np.array(Y_test).reshape(-1)\n",
    "        y_mean = np.mean(Y_test)\n",
    "        ss_res = np.sum((Y_test - Y_Pred) ** 2)\n",
    "        ss_tot = np.sum((Y_test - y_mean) ** 2)\n",
    "        R = 1 - (ss_res / ss_tot)\n",
    "        return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57508e",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "860dd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df2.iloc[:, 0:100]\n",
    "Y_train = df2.iloc[:, 100]\n",
    "X_test = df2_test.iloc[:, 0:100]\n",
    "Y_test = df2_test.iloc[:, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f9916d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5994543894485629\n",
      "-0.5717156062803062\n"
     ]
    }
   ],
   "source": [
    "ModelA = LinReg()\n",
    "w_A = ModelA.Fit(X_train,Y_train)\n",
    "y_hat_A = ModelA.Test(X_test)\n",
    "Rsq_A = ModelA.Eval(y_hat_A, Y_test)\n",
    "print(Rsq_A)\n",
    "\n",
    "X_train_std = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test_std = (X_test - X_train.mean()) / X_train.std()\n",
    "\n",
    "ModelB = LinReg()\n",
    "w_B, res_B = ModelB.Fit(X_train_std, Y_train, alpha=0.0001, epoch=100000, method = 'Batch_GD', capture_cost = True)\n",
    "y_hat_B = ModelB.Test(X_test_std)\n",
    "Rsq_B = ModelB.Eval(y_hat_B, Y_test)\n",
    "print(Rsq_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3491d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff43ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7eb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fb9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.5994543894485629)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bc781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83760c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072efc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42f6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d290fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-35.41620788932948)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4220648",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = []\n",
    "for wi in res_B:\n",
    "    norm.append(np.linalg.norm(w_B - wi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a0055988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(norm)\n",
    "plt.xlabel(\"Iteration (t)\")\n",
    "plt.ylabel(\"Norm\")\n",
    "plt.title(\"Convergence of Gradient Descent towards Analytical Solution\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07757521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
